{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"300\" src=\"https://data-services.hosting.nyu.edu/assets/libraries_short_color.png\" alt=\"NYU Libraries Logo\">\n",
    "\n",
    "# Getting Started with Python Pandas\n",
    "\n",
    "**Nicholas Wolf**<br/>\n",
    "[ORCID 0000-0001-5512-6151](https://orcid.org/0000-0001-5512-6151)\n",
    "\n",
    "This lesson is licensed under a [Creative Commons Attribution-NonCommercial 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This class is meant to be an overview of using Python Pandas for those who have never used it before, or who have made some progress but could use further guidance. Some background in working with tabular data is helpful, but not required. We'll focus on the following goals:\n",
    "\n",
    " - Understand the building blocks of a Pandas dataframe\n",
    " - Know how to make a dataframe and how to load it with data\n",
    " - Filtering, selecting, and other common operations needed to focus on a subset of a dataframe\n",
    " - Updating values\n",
    " - Table joins and merges\n",
    " - Exporting a dataframe to a saved file\n",
    "\n",
    "**Materials**\n",
    "\n",
    "You can always find this notebook (in non-executable form) at <a href=\"https://nyu-dataservices.github.io/startingpandas\">https://nyu-dataservices.github.io/startingpandas</a>.\n",
    "\n",
    "We'll be using this Jupyter Notebook as a basis for the lesson. You can access the code/notebooks for this class in the following ways:\n",
    "\n",
    "*If you do not have Python, Pandas, and Jupyter Notebooks installed:*\n",
    "\n",
    " - Use the Jupyter Notebook available on our course JupyterHub instance here: https://tutorials-1.rcnyu.org. Navigate to the \"shared\" directory, then to RDM_StartingPandas, and then open the file \"session-notebook-complete.ipynb\". You will need to change the kernel to \"RDM_Main.\" You will then be able to run and edit the code, but not save any changes. We will go over how to access this browser-based notebook in class.\n",
    " \n",
    "*If you have Jupyter Notebooks and Pandas installed on your laptop:*\n",
    "\n",
    " - You can clone this repository and open the \"session-notebook-complete.ipynb\" file:\n",
    "\n",
    "<code>git clone https://github.com/NYU-DataServices/startingpandas.git</code>\n",
    "\n",
    " - Or you can download the materials by visiting <a href=\"https://github.com/NYU-DataServices/startingpandas\">https://github.com/NYU-DataServices/startingpandas</a>. Select the green code/clone button at the top, and select \"Download Zip.\" Once you have the downloaded zip package, unzip it and open the \"session-notebook-complete.ipynb\" file.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using Pandas...and NOT using Pandas\n",
    "\n",
    "Pandas can be a powerful tool, especially for those using it who have a background in other statistical software and are looking for a means to work with tabular data. But it isn't the only (or in some cases even the best) means of dealing with data munging or data analysis in Python, particularly for large data.\n",
    "\n",
    "For example, note the respective size of these two Python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a 900 x 900 table of integers and store it as a simply Python list of list-rows:\n",
    "\n",
    "list_lists = [list(range(0,900)) for i in range(0,900)]\n",
    "\n",
    "# Make a Pandas dataframe out of that same table\n",
    "\n",
    "df_list_lists = pd.DataFrame(list_lists)\n",
    "\n",
    "# Note the size difference in memory of these two objects. This is size in bytes\n",
    "\n",
    "print(list_lists.__sizeof__())\n",
    "print(df_list_lists.__sizeof__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python list of lists is considerably smaller in bytes than the dataframe.\n",
    "\n",
    "Unsurprisingly, users experience periodic issues in reading large tables into a Pandas dataframe because of this overhead. A sense of these problems and common workarounds can be found on this [Stack Overflow thread](https://stackoverflow.com/questions/11622652/large-persistent-dataframe-in-pandas).\n",
    "\n",
    "On the other hand, our Pandas dataframe will start to outperform Python loops to modify data as size as our table/matrix gets larger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the fourth column of our list of lists\n",
    "\n",
    "def update_list(list_lists):\n",
    "    new_list_lists = []\n",
    "    for row in list_lists:\n",
    "        new_list_lists.append(row[0:3] + [row[3] * 3] + row[4:])\n",
    "    print(new_list_lists[0][0:5])\n",
    "    \n",
    "\n",
    "print(list_lists[0][0:5])\n",
    "\n",
    "%timeit -n 1 -r 1 update_list(list_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 1 -r 1 df_list_lists[3] = df_list_lists[3].apply(lambda x: x*3)\n",
    "\n",
    "df_list_lists.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't forget: a Pandas dataframe is a special kind of two-dimensional array, and arrays excel at performing matrix-based transformations\n",
    "\n",
    "In other words, if you simply need a container to \"hold\" your data, a lot of times a simple core Python structure is great. But if you need to do full-table transformations, quick statistics, advanced statistics, and table relational joins, then Pandas is a great option.\n",
    "\n",
    "**It is essential if you want to do the steps above AND you have non-uniform data types.**\n",
    "\n",
    "Unlike another commonly used matrix library, numpy, Pandas dataframes accommodate tables/matrices that mix integers, strings, and other data types. (Pandas also shares some underlying code with numpy.)\n",
    "\n",
    "## 2. Building a Dataframe: Series\n",
    "\n",
    "To understand how a dataframe works in Pandas (or any other environment) we can think of the multiple ways we can assemble a two-dimensional table like this:\n",
    "\n",
    "<img align=\"left\" width=\"300\" src=\"pandas_table_1.png\" alt=\"A two-dimensional table illustrating how data might be organized\"><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "Now we might conceive of this table as consisting of four rows, or observations, with each row consisting of elements that are ordered so that they align with a column location that tells us what the value is for any given variable.\n",
    "\n",
    "But we also might think of a table as consisting of vertical uniform-length columns, each representing the measurement of single variable across the same number of observations, that are then assembled by stacking them side-by-side:\n",
    "\n",
    "<img align=\"left\" width=\"350\" src=\"pandas_table_6.png\" alt=\"An image showing how a table is also built out of uniform columns\"><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "In Python terms, we might think of rows and columns in a table as having some \"dictionary-like\" qualities, and some \"list-like\" qualities. For example, the elements of a row can be conceived of values that are each paired with a key corresponding to our column headers (or variables):\n",
    "\n",
    "<img align=\"left\" width=\"800\" src=\"pandas_table_3.png\" alt=\"Image showing how we might think of a table row as an equivalent of a Python key-value dictionary\"><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "We can also think of the rows and columns as having an index order, like a Python list, so that we might slice and retrieve a column or row (or a value using both column and row) using its index:\n",
    "\n",
    "<img align=\"left\" width=\"350\" src=\"pandas_table_2.png\" alt=\"Image showing how we might think of a table as having index ordered rows and columns\"><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "And we can think of each column as being like a Python list, with an order so that once again we might access individual values and stacked next to each other to form a table:\n",
    "\n",
    "<img align=\"left\" width=\"600\" src=\"pandas_table_4.png\" alt=\"Image showing how we might think of a table as consisting of several uniform-length lists placed next to each other\"><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Pandas Series object\n",
    "\n",
    "Recognizing these hybrid dictionary- and list-like qualities of the components of a two-dimensional array, the building block for Pandas dataframe is the Series object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might create a Series from a list:\n",
    "\n",
    "list_series = pd.Series([\"student1\", \"student2\", \"student3\", \"student4\"])\n",
    "\n",
    "list_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our resulting Series has an index, and looks like a 4 x 1 (4 rows x 1 column) array. Let's add a name so that we understand what this column/vector of values refers to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_list_series = pd.Series([\"student1\", \"student2\", \"student3\", \"student4\"], name=\"student_name\")\n",
    "\n",
    "named_list_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Series can be sliced by index location, much like a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_list_series[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_list_series[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Great! But in and of themselves, a Series object isn't that helpful. But putting several together gives us a dataframe. We can do this by instantiating a DataFrame object which has been passed a dictionary of Series, i.e. one or more Series objects identified with a key that will serve as the column header:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_series = pd.Series([1990, 1991, 1992, 1993])\n",
    "\n",
    "pop_series = pd.Series([1.5, 1.6, 1.8, 2.0])\n",
    "\n",
    "population_table = pd.DataFrame({\"year\":year_series, \"pop\":pop_series})\n",
    "\n",
    "population_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Pandas automatically builds for us a row index, highlighted in bold, on the lefthand side. If we had failed to provide column names, it would have used index numbers to label them.\n",
    "\n",
    "That's all we need to know about the Pandas Series object to get started. Mostly, this is helpful so that we know that when we operate on a single column sliced from a dataframe, we are operating on a Series object.\n",
    "\n",
    "## 3. Loading a DataFrame\n",
    "\n",
    "We have several options for how to make a dataframe and start working in Pandas:\n",
    "\n",
    "1. We can load a tabular data file and allow Pandas to parse it as a dataframe\n",
    "\n",
    "2. We can instantiate an empty dataframe and append rows or columns in the form of Series objects\n",
    "\n",
    "3. We can transform a Python complex array (such as a list of lists or a list of dictionaries) into a dataframe\n",
    "\n",
    "No matter which approach is taken, I recommend taking some time to set the various parameters of the pd.DataFrame object so that your work on the dataframe later has expected results. This includes setting column names, column order, data types of variables, and (when reading from file) encoding.\n",
    "\n",
    "Here are examples of all three:\n",
    "\n",
    "#### Load from CSV/Excel/TSV, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading from a character-delimited file using explicit settings for delimiter, \n",
    "# encoding, data types, and using column names to order the resulting dataframe\n",
    "\n",
    "df_from_csv = pd.read_csv(\"water-consumption-nyc-csv-example.csv\", delimiter = \",\",\n",
    "                          header = 0, names = [\"Year\",\"NYC_Pop\", \"Consumption\",\"PerCapita_Consumption\"],\n",
    "                          dtype = {\"Year\":int, \"NYC_Pop\":int, \"Consumption\":int, \"PerCapita_Consumption\":int})\n",
    "\n",
    "# We can use .head() and .tail() to preview just a portion of a dataframe.\n",
    "# Pass as an optional parameter the number of rows you wish to see\n",
    "\n",
    "df_from_csv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearranging column order\n",
    "\n",
    "df_from_csv = df_from_csv[[\"Year\",\"NYC_Pop\", \"PerCapita_Consumption\", \"Consumption\"]]\n",
    "\n",
    "df_from_csv.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Populating an empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an empty dataframe, then add 1 or more rows, each as dataframes with columns aligned with the original\n",
    "\n",
    "empty_df = pd.DataFrame(columns = [\"Year\",\"NYC_Pop\", \"PerCapita_Consumption\", \"Consumption\"])\n",
    "\n",
    "newrows = pd.DataFrame([\n",
    "            {\"Year\": 1984,\n",
    "            \"NYC_Pop\": 8102100,\n",
    "            \"PerCapita_Consumption\": 188,\n",
    "            \"Consumption\": 1302\n",
    "            },\n",
    "            {\"Year\": 1985,\n",
    "            \"NYC_Pop\": 8902100,\n",
    "            \"PerCapita_Consumption\": 176,\n",
    "            \"Consumption\": 1203\n",
    "            }\n",
    "        ], columns = [\"Year\",\"NYC_Pop\", \"PerCapita_Consumption\", \"Consumption\"])\n",
    "\n",
    "empty_df = empty_df.append(newrows, ignore_index=True)\n",
    "    \n",
    "empty_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selecting/filtering rows and columns from a dataframe\n",
    "\n",
    "One of the most common operations we need to do with dataframes is filter or select a subset of observations, or perhaps a set of columns, to use for analysis. Let's start by looking at how we can grab just one or more columns from a dataframe.\n",
    "\n",
    "#### Subsetting one or more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_csv[\"Year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can grab one or more columns either by column index location, or by column name\n",
    "\n",
    "sub_df_from_csv = df_from_csv[\"Year\"]\n",
    "\n",
    "sub_df_from_csv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that a single column subsetted from a dataframe is a Series\n",
    "\n",
    "type(sub_df_from_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting multiple columns. Note that we pass multiple column names as a list\n",
    "\n",
    "sub2_df_from_csv = df_from_csv[[\"Year\", \"NYC_Pop\"]]\n",
    "\n",
    "sub2_df_from_csv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling multiple columns gives us a dataframe!\n",
    "\n",
    "type(sub2_df_from_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting rows using slice notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub3_df_from_csv = df_from_csv[2:4]\n",
    "\n",
    "sub3_df_from_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "### Challenge\n",
    "\n",
    "What snippet of code would you use to slice out the 12th, 13th, and 14th rows and the columns \"NYC_Pop\" and \"PerCapita_Consumption\"?\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Answer\n",
    "\n",
    "df_from_csv[  [\"NYC_Pop\", \"PerCapita_Consumption\"]    ][12:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering using index location: .iloc()\n",
    "\n",
    "The row and column extraction operations are only useful up to a point -- usually you want to select a subset of your dataframe using some kind of complex criteria. We have two main means of doing this: by index location of columns and rows, and by label name and/or some kind of Boolean test of the value in any given \"cell.\" A third means combines the two. We start by looking at filtering/selecting by index location using **iloc()**.\n",
    "\n",
    "The concept here will be familiar to those comfortable with list slicing, or with how a matrix can be extracted in numpy. The pattern is:\n",
    "\n",
    "<code>dataframe.iloc[row_index_start : row_index_index_stop, column_index_start : column_index_stop]</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First two rows, third and fourth column only\n",
    "\n",
    "df_from_csv.iloc[0:10, 2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All rows, second column only\n",
    "\n",
    "df_from_csv.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third row, all columns\n",
    "\n",
    "df_from_csv.iloc[2:3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering using index location: .loc()\n",
    "\n",
    "We can also select using the same pattern (colon separated spans, a comma between the row and column decisions), but use a label or Boolean filter to subset.\n",
    "\n",
    "The pattern is:\n",
    "\n",
    "<code>dataframe.loc[row_label(s), column_label(s)]</code>\n",
    "\n",
    "or, for a Boolean:\n",
    "\n",
    "<code>dataframe.loc[ True/False test for row values in a column, column_label(s)]</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all rows, but only the Year and NYC_Pop columns:\n",
    "\n",
    "df_from_csv.loc[:, \"Year\":\"NYC_Pop\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also have an option to pass two or more column labels, even if non-adjacent, to select columns we want:\n",
    "\n",
    "df_from_csv.loc[:, [\"Year\", \"Consumption\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand that the first parameter, the \"row\" label, is actually the column that serves as an index.\n",
    "# In all of our examples so far, we've been using a Pandas-built numerical index. Thus:\n",
    "\n",
    "df_from_csv.loc[0, [\"Year\", \"Consumption\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, keep in mind that we can set one of our columns to be an index, \n",
    "# enabling us to use the column values as label\n",
    "\n",
    "df_newindex = df_from_csv.copy().set_index(\"Year\")\n",
    "\n",
    "df_newindex.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newindex.loc[1981, :].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important! Boolean Tests in Filtering\n",
    "\n",
    "Lastly, we have our Boolean means of filtering using .loc(). Note that we want to access a column within our brackets and perform some kind of True/False Boolean test. We can do this using the dataframe.columnName syntax (as long as our column names do not contain spaces. If they do, you must use dataframe[\"columnName\"] instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_csv.loc[df_from_csv.Year < 1983]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, same thing:\n",
    "\n",
    "df_from_csv.loc[df_from_csv[\"Year\"] < 1983]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chain multiple Boolean tests together to get some pretty sophisticated filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we use | (pipe) and & for \"OR\" and \"AND\" Booleans, respectively. We MUST use parentheses \n",
    "# to separate out each Boolean to be evaluated. \n",
    "\n",
    "df_from_csv.loc[(df_from_csv.Year < 1983) | (df_from_csv.Year == 1989)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using more than one AND/OR and added parentheses to clarify\n",
    "\n",
    "df_from_csv.loc[((df_from_csv.Year < 1983) | (df_from_csv.Year == 1989)) & (df_from_csv.Consumption > 1320)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, slightly hidden from us is the matrix logic used behind the scenes to make all of this happen.\n",
    "\n",
    "In reality, the Boolean tests inside the parentheses return True/False versions of the original dataframe in which each cell is set to True or False based on the condition you have indicated. It is this True/False matrix that is then used to mask the original dataframe.\n",
    "\n",
    "We can test this by simply evaluating the Boolean statement itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_csv.Year < 1983"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick note: deprecated versions\n",
    "\n",
    "If you come across examples on Stack Overflow using .ix[] to filter/select data, disregard as this is now deprecated in Pandas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modifying values in a dataframe\n",
    "\n",
    "Once you are comfortable selecting subsections of a dataframe, from entire rows, columns, sub-tables, or individual single values, ways of updating those values can take a few forms. We'll concentrate on two.\n",
    "\n",
    "#### Direct setting of new value on an existing dataframe\n",
    "\n",
    "Let's say we want to update the value in the Consumption column for the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_csv.iloc[0:1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_csv.loc[0, \"Consumption\"] = 1548\n",
    "\n",
    "df_from_csv.iloc[0:1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also do quick calculated values across all rows and set the results on a new column\n",
    "\n",
    "df_from_csv[\"Consumption_per_10000\"] = df_from_csv.Consumption / 10000\n",
    "\n",
    "df_from_csv.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A very helpful approach: dataframe.apply() \n",
    "\n",
    "A great feature of Pandas is the .apply() method which enables you to apply some kind of transformative function, either one you write yourself or something you pull from Python's core library, and propagate it across any portion of the dataframe you wish to change.\n",
    "\n",
    "We can start with an example using a Python core function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1.6\n",
    "round(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_csv[\"Consumption_per_10000\"] = df_from_csv[\"Consumption_per_10000\"].apply(round)\n",
    "\n",
    "df_from_csv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative using a function you write yourself.\n",
    "\n",
    "def check_if_high(in_val):\n",
    "    if in_val > 200:\n",
    "        return \"Y\"\n",
    "    else:\n",
    "        return \"N\"\n",
    "    \n",
    "df_from_csv[\"Consumption_high\"] = df_from_csv[\"PerCapita_Consumption\"].apply(check_if_high)    \n",
    "\n",
    "df_from_csv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a final example, using a quick lambda function\n",
    "\n",
    "df_from_csv[\"Total_sum\"] = df_from_csv[\"Consumption\"].apply(lambda x: x+1000)\n",
    "\n",
    "df_from_csv.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Table joins and table concatenation\n",
    "\n",
    "Helpfully, Pandas offers some SQL-like means of joining (i.e. in a SQL sense: matching records on common keys, yielding a wider table) and performing the equivalent of a SQL union (i.e. concatenating one table of the same structure and columns to another, yielding a longer table).\n",
    "\n",
    "These are known as a \"merge\" and a \"concat\" in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a second table with a plan to use \"Year\" as our common key\n",
    "\n",
    "nyc_pop_density = pd.DataFrame([[1979,8.9],[1980,8.1],[1981,7.4],[1982,7.2],\n",
    " [1983,6.5],[1984,6.5],[1985,6.0],[1986,5.8],\n",
    "[1987,5.5],[1988,5.4]], columns=[\"pop_year\",\"density_per_5000\"])\n",
    "\n",
    "nyc_water_consumption = df_from_csv.loc[df_from_csv.Year < 1989]\n",
    "\n",
    "nyc_pop_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_water_consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a join (\"merge\") with the year column;\n",
    "# we'll do a left-hand inner table join, meaning we will keep all records in the water consumption table (lefthand)\n",
    "# and only records that match from the right-hand table (the pop density). As it happens, there is a 1-1 match\n",
    "\n",
    "merged_df = pd.merge(nyc_water_consumption, nyc_pop_density, how=\"left\", left_on=\"Year\", right_on=\"pop_year\")\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we have a second dataframe with the same type of data, using the same columns.\n",
    "# We perform a concatenation in which the second dataframe becomes additional row on the original\n",
    "\n",
    "additional_df = pd.DataFrame([[1989,7367819, 189, 1456, 0, \"Y\", 2456],[1990,7406718, 192, 1345, 0, \"Y\", \"2345\"]],\n",
    "                         columns=[\"Year\",\"NYC_Pop\",\"PerCapita_Consumption\",\n",
    "                                  \"Consumption\",\"Consumption_per_10000\",\"Consumption_high\",\"Total_sum\"])\n",
    "\n",
    "additional_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting dataframe will preserve the original row indices of the original respective dataframes\n",
    "# To reset the index, we pass an additional parameter: ignore_index=True \n",
    "\n",
    "concat_df = pd.concat([nyc_water_consumption, additional_df], ignore_index=True)\n",
    "\n",
    "concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary statistics and data quick views\n",
    "\n",
    "There are a few key methods that we can call on a Pandas dataframe to help us get a sense of its shape and contents quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check its size. This dataframe has 38 rows and 7 columns\n",
    "\n",
    "df_from_csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls; in this case, nulls in any row, any column.\n",
    "\n",
    "df_from_csv[df_from_csv.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any given column is unique\n",
    "\n",
    "df_from_csv.Year.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "\n",
    "df_from_csv.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single column max, min, value counts\n",
    "\n",
    "print(nyc_water_consumption.Consumption_high.value_counts())\n",
    "\n",
    "print(nyc_water_consumption.NYC_Pop.max())\n",
    "\n",
    "print(nyc_water_consumption.Consumption.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. More help\n",
    "\n",
    "1. I love this plain explanation of filtering/selecting and refer back to it often: [https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/](https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/)\n",
    "\n",
    "2. Official Pandas documentation, recently re-newed: [https://pandas.pydata.org/pandas-docs/stable/index.html](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
    "\n",
    "3. Pandas's own 10-minute quick start: [https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html#min](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html#min)\n",
    "\n",
    "4. Software Carpentries Pandas lesson (as part of general Python instruction): [https://swcarpentry.github.io/python-novice-gapminder/08-data-frames/](https://swcarpentry.github.io/python-novice-gapminder/08-data-frames/)\n",
    "\n",
    "5. NYU Data Services Quantitative guide for merging/joining datasets: [https://guides.nyu.edu/quant/merge](https://guides.nyu.edu/quant/merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_csv[12:15][[\"NYC_Pop\",\"PerCapita_Consumption\"]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RDM_main]",
   "language": "python",
   "name": "conda-env-RDM_main-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
